# Sebastian Kung 
_**Achieving Secure Deployment of High-Stakes Software**_  

Sebastian is a physics student at the university of Zurich, and works for a hardware wallet company. He contributes to various cryptocurrency related projects.  

**Achieving Secure Deployment of High-Stakes Software**  
_Sebastian Kung, University of Zurich_  
[youtu.be/5xust7eXpDM](https://youtu.be/5xust7eXpDM)  

MoneroTalk w/ Sebastian Kung on Reproducible Builds at MoneroKon 2019! - [youtu.be/mXyNrsfaLpE](https://youtu.be/mXyNrsfaLpE)  

_**Abstract**_

A tampered release of Monero could have catastrophic consequences. Widespread methods to secure releases, like publishing a signed checksum of the binary, do not offer protection against rollback attacks, the maintainer releasing a binary containing malicious code, or users not checking signatures. I will present an introduction to reproducible builds, cross platform toolchains, update systems and user-friendly verification methods to ensure binary integrity.

_**Transcription**_

So I will be talking about software deployments, reproducible builds, and then I'll give some comments on why reproducible builds are still not enough. We need even better systems.

I'll also be talking about software deployment in a model where we do an upstream releases. So releases as done in Monero where the main developer tags the software at some points then compiles this and ships it to the public. And by doing so he would, well, if everything would go perfectly just launch a beautiful spaceship but for the user sides, he might just, oh shoot. How do I go back? But from the user sides, he might have received a big old can of worms. So the question is, how do we go from source code to program running on a users machine?

So we have some source codes we compile it with the toolchain of our choice be [GCC](https://gcc.gnu.org/), [LLVM](https://llvm.org/), or some proprietary toolchain and then publish it on either an app store, website like [SourceForge](https://sourceforge.net/) or self-hosted websites. Now if you deploy closed source software it is all very very easy. Basically, all you need to do is compile, publish it somewhere where you can be held somewhat accountable and then bring some users to the group and make them use it. And make some money with it.

This is obviously a very very simplified system and something that probably every single person in this room strongly dislikes because the user has to completely trust the binary blob that he is receiving. Not just sell, steal, or otherwise corrupt his data to exactly do what is specified to do by the publisher and more importantly he also cannot comfortably study, tweak, or share any aspects of that program. 

He also has to completely trust its publisher not to mess around with the binary. So for example if you publish something on SourceForge, let's say five years ago or something, they would embed adware into the Windows installer and users would then get software that they did not intend to get. Also, we are community-driven project so you can't really hold the community accountable, even though we all obviously strive to perfect the quality of the software that we are shipping.

On free software deployment where the user is able to study, tweak, share, and re-publish as he pleases, he obviously can also re-publish a tweak version of the software that contains malware or adware. The easiest solution for this is obviously - what is done or what has been done up until very recently in Monero - is ship the binary with the developer signature, and then check that, that developer is inside some web of trust. Yet he's a public person or he is inside the PGP strong set or whatever you choose as your [inaudiblae]() model. [Getmonero.org](https://web.getmonero.org/) also has some [very easy instructions of how to verify this](https://web.getmonero.org/resources/user-guides/verification-allos-advanced.html). And the file that contains all these hashes looks like this so it's very easy to actually verify that a binary that you have received has the correct checksum.

But this still kind of leaves the question open, how do we take care of updates? So in app stores, for example, updates are being taken for you more or less automatically. The developer can just publish a new version to the app store and your phone will then updates the app automatically. Monero in the other hand - we do have an update system but we have to make sure that it also checks for some weird things like that old versions aren't replyed again which will still have valid checks on the [signatures](https://web.getmonero.org/resources/moneropedia/signature.html) but might contain exploits that are dangerous to the user. So we need some way to take care of this toxic waste. 

The way this is being done usually is through the [Update Framework or TUF](https://theupdateframework.github.io/), short. And this spells out very precisely how to roll out updates securely. Beautiful closed source software, compiled software, just the script, whatever. Basically, the user keeps a record of the binary checksums, signatures, and versions in a separate file. That separate file is not [inaudible] separately from the actually binary. Then checks are made for attacks that no time warping has occurred and this gives you reasonable security. Monero's update system was also evaluated against TUF in Monero [Issue #4958](https://github.com/monero-project/monero/issues/4958). Please take a look at this, Jasonhcwong, the producer of the Monero blocks made some really great points on there. 

Next to more or less in [inaudible] to TUF, Monero also implements [dnssec](https://en.wikipedia.org/wiki/Domain_Name_System_Security_Extensions) with additional fallbacks to protect against [spoofing attacks](https://www.veracode.com/security/spoofing-attack) when you have automatic updating available.

So I'm going to make a little detour now and talk about static and dynamic linking of compiled software. So most software that is shipped in Debian for example, is shipped as a dynamic binary. Meaning it loads its required libraries from the system at run-time. You can to a certain to degrees freely swap out these shared libraries that you are [inaudible] executable links to. As long as they follow a common ABI. So for example, if you publish a binary on Debian with apps, so for example [monerod](https://src.getmonero.org/resources/user-guides/vps_run_node.html), and you then check what its dependence soft links are, you can do so with ldd and will give you a whole list of dependencies. This also gives you quite some transparency because you actually see what the developer used to compile your executable. 

But there's obviously a big downside to it. Mainly that you need to manage all these shared libraries yourself and if one of them goes missing obviously your executable won't run properly. This also counts for [inaudible] missing but also having version mismatches, etc. And if you deploy for example an application like monerod to Windows, you have to include all the dynamic libraries with the binary directly. So these are non-system dependencies that are not being updated regularly on Windows, so you can't rely on Windows to patch your dependencies for you. So for example, if OpenSSL is having an exploit, monerod will have to re-compile everything and ship it with that new version of OpenSSL.

So, in general, this does require a lot of overhead to package since you ultimately want to target a very diverse of Operating Systems and after cater for the quirks of each of these package managers. There are some solutions to create packages inside a, I will call it a tiny file system, like nap image but they come with their own trade-offs, again. So if you, for example, generate nap image you will get a single binary but the single binary will have some shared libraries packed within it. So you again have the problem that as soon as one of the dependencies you are building with have an exploit you will have to update the entire thing again.

So since we have this problem that we are shipping for Windows, static linking seems to be the same option. So in static linking the libraries are embedded directly into the executable binary that your shipping. And that basically takes care of this problem then. But if you statically compile on Linux you have to be a bit careful. So this is basically a hack, I wouldn't call this something that's generally advertised by developers working on Linux but when you link a binary on Linux it will also dynamically link in libc. And this will always be dynamic. Libc has a quirk that it is always forward compatible so a new version of libc will run a binary that was compiled on a system with an old version of libc but not vice versa. And the solution to this is to redefine the symbols that are used in native systems inside your source code when you are compiling and to check which version of [glibc](https://www.gnu.org/software/libc/) your binary is running with you can just run objdump dash "p". And once you've done that you can run objdump dash "T" and then a binary name and grep for the libc version that you want to wrap then. And you do this by redefining the symbols in the source code with [Simba](https://simba-os.readthedocs.io/en/latest/) so every single, every single, sorry have a blackout. So for example, if you have a symbol that finds in glibc version 2.17 this will have a single symver entry inside the C library. But then when you are backtracking to old versions you again have to be careful that those aren't susceptible to an exploit. So if you are back versioning for example glop from glibc version 2.27 to 2.17, you will run into this problem because it was patched in order to remove a CVE from that occurs or that would even allow you root access to a Linux system. And if you can't ship around this you then really have to just embed this part of the CVE library directly into your source code.

I've written a [blog post](https://thecharlatan.github.io/GLIBC-Back-Compat/) about how to do this exactly and it's available when you go throughout the QR code.

So I've argued now that we should generally use static linking when we are targeting a lot of platforms. We know now how to roll out updates more or less securely, we know about checksums, and developers signatures but we still have the problem that if the developer uses precompile third-party libraries, these precompiled libraries can inject malware into the binary you are using. 

And the solution to this is you cross-compile all your dependencies from a single environment by installing the toolchains required for this on your host system. This is now done in [Monero with depends](https://github.com/monero-project/monero/tree/master/contrib/depends), [depends](https://github.com/bitcoin/bitcoin/tree/master/depends) is basically a system of make files that was authored by Bitcoin Core developer [Cory Fields](https://github.com/theuni) and these make files enable cross-compilation for Linux, Mac OS, and Windows. And then targeting a bunch of different architectures including on 32-bit, on 64-bit, and even on [RISC-5](https://riscv.org/).

What depends does is basically gets the dependencies you're building with directly from source it then verifies their checksum, compiles each of the dependencies for each architecture and operating system and installs them into a separate directory that is generated within your dependence environment. And to this tree directory, you can then lay to link to when you are compiling to your actual application. And for this, I added the make depends target and then this architecture when the system triplet to make it fairly easy to use.

To add new packages to depends is fairly easy, basically all you need to do is create a new package file, for example, here, for example, I'm showing lipsodium. You have to give it a name a version, where it should get it from source, a checksum, and then some commands to configure it correctly, to compile it correctly, and then to install it inside this depends system root. A lot of this - packages when you cross-compile them - require additional patches. These have a variety of reasons, for example, in lipsodium, the developer decided that in the version string he's just going to include a white space and if you then pass that into [GCC](https://gcc.gnu.org/) for some versions it will obviously complain that there is a weird white space in between. But there's also a lot of new packages that are completely outdated where the last release was done when for example ARM 64-bits architectures weren't even defined properly and you basically need to patch their entire source code in order to make them or in order to make that library compatible for ARM-64.

So the developer has now a safe set of libraries that he can compile from. He can update to users more or less securely. The users can verify that the update was received securely but the users still have to completely trust the developer that he actually compiled the software correctly. So when a developer does release, he can include malware at compile time either willingly or subvertingly by someone having compromised his system. Typically when you compile or build software on an application, two compilation runs of the same source code on two different systems, will not produce the same binary. So you will get functionally-- functionally you will get the same binary like will have more or less the same functionality but will have different checksums. This is because when you compile software the compiler will embed timestamps, system paths, dependency versions, debug symbols, and a lot more junk into your actual binary. And the solution to ship around this is to run the compilation inside a container and have a common recipe for everybody to run this compilation run in exactly the same fashion every single time. And the process of doing this is called running [Reproducible Builds](https://reproducible-builds.org/).

Bitcoin or Monero use [Gitian](https://github.com/monero-project/monero/tree/master/contrib/gitian) to run Reproducible Builds. Gitian is basically a collection of scripts that start a container and then run the builds as specified by the developer inside this container. Users can choose between kvm, lxc, and docker as a backend for this virtual environment. And the build details are parsed from a dot yml file that you can pass through the Gitian scripts. This file includes all the compilation steps, the fake time set up that is required to set the system time to zero at every time you're starting a new compilation run and the toolchain installation.

In Bitcoin and Monero this is currently bootstrapped from a container running an Ubuntu 18.04 image, and it obviously uses depends to compile all the dependency libraries. The decision to use Ubuntu 18.04 was done with some consciousness. So Ubuntu is not the freest of distributions but it does have some repositories pre-configured that have very modern versions of GCC. And more importantly, it also uses apt where the GCC compiler switch, to a certain extent, is reproducibly compiled. Once you are finished the compilation ran in Gitian the resulting binaries are copy back to the host system. And then Gitian produces this manifests file containing the checksums of the tools and the libraries used - and obviously the binaries and applications you are producing. This file is in cross-compared to manifest that other developers have built and as soon as a certain threshold of people have produced the same manifest files a release is then pushed out to the public.

So, for example, these are the top few lines of the Monero 0.14.1 release manifest file containing new checksums of the Linux binaries. You can run this Gitian set up yourself it's fairly easy. Basically, what you need to do is install some virtualization environment, for example, docker, then copy the [Gitian built py script from the Monero repository](https://github.com/monero-project/monero/tree/master/contrib/gitian#manual-and-building) and run these two commands.

To inspect non-reproducibility sources inside software I prefer using a tool called [Diffoscope](https://diffoscope.org/) just because it's very very easy to use and it's also a very small tool. It's basically a thin wrapper around objdump and readelf that just formats the output nicely. There are plenty more tools to analyze non-reproducibility sources and you can all of them on [reproducible-builds.org](https://reproducible-builds.org/).

So right before we released 0.14.1 a source of non-reproducibility was found when I compared my binary to the one of Howard Chu. And basically, the only difference was that on Howard Chu's machine he had some more objects in his good repository and the get short hash would then include one character more than on my system where the git repository had fewer objects. So it's very very subtle changes that kind of ready have a huge effect on the actual checksum in the end. We fixed this problem and now builds Monero already finally reproducible for every single platform we are releasing in.

However, reproducible builds are still not enough, it basically only guarantees that all binaries are equally compromised. So we still have to invest a lot of time into review of each of the dependencies we're using and obviously of the code we are writing. A second problem we have is that we need to completely trust the toolchain which most of the time just gets shipped as a binary blob. So even for example if you compile ARCH Linux from source you will start with some generic GCC compiler. Carl Dong gave a really good talk about this two weeks ago at [Breaking Bitcoin](https://youtu.be/I2iShmUTEl8). And this general concept of distrusting your toolchain is called the [Trusting Trust](https://www.archive.ece.cmu.edu/~ganger/712.fall02/papers/p761-thompson.pdf) problem and was first sort of turned by Ken Thompson in a keynote address when he received a price for his work on [UNIX](http://www.unix.org/what_is_unix.html). Basically, the problem is that low level compilers in the bootstrapping process of your toolchain can inject code into your high-level compilers. So what might be the case is that if we couldn't have started some time ago in history - let's say somewhere in, sometime the 80s - and we have started this bootstrapping process with a compromise version of GCC our entire toolchain might have been compromised all this time and we wouldn't be all the wiser. 

So what we need to have again is full bootstrapability of our toolchain. And this is where [Guix](https://guix.gnu.org/) comes in. So Guix is still a fairly new Linux distribution that packs a functional package manager of the same name. And this functional package manager has the ability to bootstrap from a very small set of binaries. And the ankle is going to be that at some point in time we'll have a few lines of assembly codes in a few different architectures and from these few lines, we can then compile an entire Linux system. 

I will probably start working on Guix integration next and once that is done deploying and verifying will be even more simple than the system we have currently. Basically, just run Linux and execute one or two commands.

Before I finish I quickly want to thank iDunk, Howard Chu, ph4r05, mooo and Surae for supporting me in for the work I did over the past year and especially to Surae for organizing the conference.
