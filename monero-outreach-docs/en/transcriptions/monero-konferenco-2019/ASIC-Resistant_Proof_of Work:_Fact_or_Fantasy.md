# Howard Chu
_**ASIC-Resistant Proof of Work: Fact or Fantasy?**_  

Software engineer and genius of optimization, with proven track record of world's fastest multiprocessor TCP stack, world's fastest Appletalk stack, world's fastest LDAP server, faster than realtime speech recognition, faster than realtime radar data demux/decoders, etc. 

**ASIC-Resistant Proof of Work: Fact or Fantasy**  
_Howard Chu, Symas Corporation_  
[youtu.be/4Hkd-n1W_e4](https://youtu.be/4Hkd-n1W_e4)  

[MoneroTalk](https://www.youtube.com/channel/UC3Hx81QYLoEQkm3vyl4N4eQ) w/ Fluffy Pony & Howard Chu at MoneroKon 2019! - [youtu.be/VyuKbwQvtRw](https://youtu.be/VyuKbwQvtRw)  

_**Abstract**_

The original CryptoNote whitepaper makes the case for egalitarian mining and defined the CryptoNight PoW algorithm to achieve this objective. Time and technology have progressed, and specialized mining hardware manufacturers have conquered all of CryptoNight’s technical obstacles. The debate over whether ASIC-resistance is actually achievable or even desirable continues, but the Monero Project has so far continued to uphold this principle of egalitarian mining. This talk will investigate the question of ASIC inevitability and what approaches we’ve explored to mitigate the advantages of fixed-function mining ASICs. It will also give an introduction to RandomX, the new PoW algorithm we’ve proposed for Monero, based on the lessons we’ve learned so far.

_**Transcription**_

Well, I'm Howard Chu, a founder of Symas Corporation. We actually celebrated our twentieth anniversary just last week, and I've been writing software for something like forty years now. I've been part of open source from the beginning of open source, I've worked on all the GNU utilities, compiler, debugger, linker, texinfo, all that stuff. I not only have software on pretty much every computer on earth, I also have software that's run in orbit, never crashed. A few things that I've done, noteworthy, over time. I have a fondness for fast software. Security: I've also done a lot with cryptography over time, secure authentication and various security systems.

OK, so, to the meat of the matter. When we're talking about ASIC resistance, what does it even mean, why does it matter, and then how do we get there?

So, just to set the backdrop for all of this, where is all this discussion coming from, Monero, how's just based on the CryptoNote protocol, designed somewhere around 2013, 2014. You can see, just, if you read through the whitepaper, that it's a strong reaction to observing flaws in the design of Bitcoin. You can see that the Bitcoin notion of pseudonymity just isn't adequate to protect its users. The fact that the Bitcoin software has hardcoded constants in it that have severe impacts on its scalability was very obvious already. And while the design of Bitcoin was intended to be decentralised, the reality was that there was heavy centralization going on in the Bitcoin ecosystem as early was 2013, 2014.

So I dug up these graphs. Just an example of the hash rate distribution of miners in 2013. And you can see the potential for a 51% attack is really high. It would take one of two pools to collude and your network is toast.

And then you can also see the evolution of mining technology. You can see where, in the very beginning, the very first year of its life, it was all just PCs, CPUs, running. And then suddenly people discovered hey, GPUs are really good at SHA-2, and so the GPU era begins and things really take off, and then hobbyist ASIC designers get into the game and then towards the end it's all the professional very large commercial scale ASIC designers.

And the very first ASIC, by today's standards, is just a very modest improvement, but that was already a fifty times improvement in efficiency. Today, nobody dreams of mining Bitcoin on their PC, because the ASIC advantage is on the order of millions. Millions of time more efficient than a CPU. The experience that we see from observing Bitcoin is that, "hey look at this: when you start depending on specialized hardware, you find that it automatically promotes centralization, which is something that we're trying to fight against the entire time, because centralized power means you have trust problems. Actually, Kristy mentioned this before as well, ASIC builders tend to use their own chips to mine for themselves rather than selling them to the general public. Or maybe they will sell them to the public, but only after the chips have become obsolete.

So, how come these ASICs can be so effective? And in this particular context, I'm still talking about the Bitcoin SHA-2 ASICs. And the reasons are fairly simple. First of all, the hardware itself is very simple, it's designed to do only a single thing, so it's fixed function hardware, it only has to do one thing and the thing that it does is fairly trivial. The SHA-2 algorithm was actually designed to be easy to compute. So the amount of work you're trying to accomplish there is very simple. And the algorithm itself is braindead easy, it runs in straight line, there's no decision points in it, you start at one side, you come out the other side, and that's it.

And so if you look at the hardware for SHA-2, you find that it's got about a dozen components, it's really simple stuff. I don't expect you to read the circuit diagram on the right, but, trust me, it's just a couple of add operators, a couple of registers, and these things are trivial in integrated circuits.

Alright, So, we find there's no decision making, there's really no branching, and 100% of hardware in that circuit is devoted to crunching your answer, there's nothing wasted on memory or timing or anything else. And when you get a circuit this simple, it's easy to put thousands of instances of it on a single chip. So that's the environment that we have today with Bitcoin mining.

Alright, so, again, the CryptoNote protocol was designed in reaction to this, but it wasn't the only one that tried to address this. If you look at Ethereum, Ethash also is designed to be ASIC resistant. And they both took a similar philosophical approach, the memory hard approach. And it turns out that the CryptoNote guys were a little conservative in their estimates. CryptoNote worked pretty well for about three or four years, and I would say time and technology caught up to it, and the resistance that it offered was defeated. Ethereum is still in pretty good shape with Ethash, but I would say its days are numbered as well.

So there's another attempt as well, you see the so-called multi hash algorithms, and you find that these actually offer no ASIC resistance at all, because all they've done is take a bunch of different hash algorithms and each one of those hash algorithms is still very simple, it's still on a scale of complexity as SHA-2 256, it still has no decision points inside, and when you have these, you can treat them as individual components, string them together any way you like and an ASIC can process all of that very, very efficiently.

So really, what we're coming down to now is dynamic algorithms which don't have a static flow of execution. There have actually been other ones besides RandomX and I'll talk a little bit about that. Funny enough, the very first one that I could find discussed was in the context of Ethereum, it was an idea that they tried out before they settled on Ethash, and it has some of the characteristics that we have in RandomX, but if you look at what they developed there, it was very, I would say, unfinished. They tried it as a prototype and they never really fully developed the idea.

And then, late- sorry, early last year in about March, I floated the idea of RandomJS, and we worked on it for like six months, six or eight months, before we found there were just some flaws in it that we could not fix, weaknesses in the approach. Another one that people probably have heard about is ProgPow, which Kristy's team actually developed, it's focused on the GPU approach, and in that respect it does very well, leveraging mass parallelism that you get out of a GPU. I would say that the random code that it generates is... I still consider it fairly simple.

Cryptonight-R, which is currently running on the Monero network. The reason Cryptonight-R exists is because we weren't done yet developing RandomX, and so some of the ideas that we had already arrived at in RandomX were kinda lifted out of there and inserted into Cryptonight just so we had something as a stopgap.

So, some people have questioned or criticized our decision to focus on CPUs, they say we are abandoning GPU miners, but you have to consider the global environment. GPUs are really not that big part of the picture. There's far more CPUs deployed in the world, there's far more CPUs being bought every year, and then, especially if you look at smart phones, which are the fastest growing element of consumer computing, there are 2.2 billion smart phone users in the world already, and they buy 1.5 billion new units every year. There's more of them, and they upgrade more. So, just if you look at the PC upgrade cycle, most people don't buy a new computer more than once every two or three years, whereas the majority of smart phone users are probably on the latest and greatest tech all the time. And so this is something that really factors into the kind of technology we're developing, which has kind of heavy demands on a CPU, and so the fact that there are more capable new smart phones works in our favour.

So, RandomX itself. We're talking about randomly generated machine language programs for a custom virtual machine that we designed ourselves. The idea here is any random eight bit sequence is always a valid instruction. This was one of the problems we had in RandomJS, which is that you had to generate a random program that followed the legal syntax of the Javascript language, so once you impose these syntax rules on it, it becomes much more complicated to randomly generate a program that will execute. So now we're talking about a machine language where there are no syntax rules, any random number you can generate will be a valid instruction. I call this moderate complexity, because if you look at the RandomX virtual machine compared to a real Intel or AMD CPU, our instruction set is still very small. But it does most of what we need to do, and the mix of instructions that we use in the generated programs is modelled after the types of operations that you see in real life user programs.

Just an idea of comparing features of different approaches. The first couple approaches I would say have zero ASIC resistance whatsoever. The next couple that come after that, they all use memory hardness as their only defence, and now we've got these dynamic algorithms that try and use both random code as well as other operations. In RandomJS and in RandomX we also use floating point math which a lot of other algorithms have avoided, and avoided for good reasons, because it's easy to get into floating point math and come up with results that differ due to rounding errors, different rounding policies, so it's a tricky thing to be able to do floating point math in a proof of work algorithm where you need everybody to get the exact same answer, but I think we handled that.

And so the idea here, and the reason that we take this approach is that we're trying to fully leverage what CPUs are good at, and what CPUs are good at is running a broad variety of code. And this is something that if you try building an ASIC to do the same thing, you're going to wind up building a CPU. And again, we're focusing on CPUs simply because they are more accessible, also because CPU instruction sets tend to all look about the same. They all have the same set of integer math operations, they all have branch instructions, whatever, and so it's easy for us to define a subset that will run well on any CPU you can imagine. So we can run this on ARM, we can run this on Intel and AMD, we can run this on PowerPC and whatever else you can throw at it. In contrast, GPUs tend to have very proprietary instruction sets, and there's not complete overlap between their functionality, so like the AMD instruction set doesn't even have one of the multiplies that we use, so it's like, OK, to target GPUs would just be a lot harder, the common subset would be much more limited. And, you know, again we have the basic realization that the work has to be dynamic. If we just have a fixed set of operations, then that can easily be hardwired into a chip, and that's a thing that we don't want.

Now, you think about what is a proof of work algorithm, at the end of the day it's just a delay loop. The goal of it is to consume time and to consume energy and it's to be as inefficient at doing this as possible. And this, you know, this was mentally jarring for me because I've spent a career building efficient software, and tuning compilers and all this to get the most work out of a system for the least amount of energy. But the interesting thing is, after you've spent all your time thinking about that, you learn fairly intimately what operations are expensive and what operations are cheap. So this does allow us to have a special perspective on how to write the most inefficient algorithm possible. And the other thing is, there's a common theme that I've run into is that privacy and decentralization are diametrically the opposite of efficiency. When you have efficient systems, they all tend to be centralized, and centralization actually improves the efficiency of most of these systems, and it's true not just in a proof of work, but if you look at network communication, how do you secure communication across the network? And, you know, we've talked about privacy a lot over this weekend, I think privacy and secrecy have been thrown around interchangeably and there's a strong difference here: privacy is only a subset of secrecy. If you have an encrypted communication channel between two endpoints, like a TLS session, what you've done here is you've kept the content of that communication channel private, but the fact that you had that communication channel is not private. People know that you communicated. So you have a private conversation but it's not a secret conversation. And for you to have privacy by itself doesn't cost too much, you know. An AES encrypted channel is fairly cheap, especially because we have hardware accelerated AES. But if you want to make something secret so that nobody knows that you've been doing it, then your cost/inefficiency goes up quite a bit, alright. The way to make a communication channel secret tends to be you have to throw a lot of noise out there so you can hide your communication under the noise, and that means you have to generate a lot more volume of traffic to hide the actual message that you wanna send. So, again, when you have efficiency, it's kinda opposed to privacy and secrecy, and so what we're trying to do with proof of work is to be inefficient, decentralized, and that still bugs my brain, but here we go.

OK, so, with RandomX, we want an algorithm that is so inefficient, that uses so much power, that we're using as much as possible of the CPU. And here we've got... this is a block diagram from an AMD Zen core. You can see at the top there's this frontend where we've got the instruction cache, decoder, branch predictor, operation cache, so this is the frontend and we've got, in the middle layer, integer operation units, floating point operation units, and then at the bottom layer we've got memory interfaces and data caches.

If you look at what we've done in RandomX, it actually uses 100% of the components of the core, but it doesn't use everything else on the chip. The CPU chip has other interfaces. It's got PCI express interfaces to all these other things, it's got an administration bus for inter-chip communication, system management, those are things that we can't really effectively utilize, just because they're so device specific. We can't account for them from one chip to another. But we can use all of the core and all of the memory interface.

So how does it all work? Well, we generate a random program, then we translate it into machine code, we execute the program and transform the output, so the most interesting part, the important part is step 3. And we want the overhead from the other steps to be as close to zero as possible because they're not really contributing to the work.

And as I mentioned before, generating a random program is kinda of a tricky part, if you're using a high level language, you have to construct it according to very strict rules otherwise the code doesn't execute and so the standard way to do this is you build an abstract syntax tree where every node of the tree is a statement in your program. And the funny thing is, you go from this abstract syntax tree to program source code, and then you feed the source code to a compiler, and the compiler parses the source code and turns it into an abstract syntax tree, which you just had, so there's a lot of redundant work going in there, and so that worked against us as far as making an effective proof of work algorithm, and if you are building an ASIC to process this, you can bypass all the redundant work, and if you bypass that work that means you've got an efficiency advantage. So this was something that killed the RandomJS idea. So the best approach that we have now is we generate just random bytes, there is no syntax, there are no construction rules. 

And then to translate this into native machine code that runs on a real CPU, we didn't want to just target x86, because in today's world, you know, ARM and other CPU architectures are major players and we don't want to leave any of those out. We use a virtual machine that we can actually translate well into any other real CPU architecture, and in order to allow us to do this as quickly as possible, we need to use simple machine level instructions that can be easily mapped onto real machine instructions, and just, there isn't time to develop an optimizing translator, you know, to analyze the code and rewrite it for particular targets, because any time you spend on that, is time you're not generating hashes.

So the actual program has to use as many CPU components as possible, and we've analyzed a lot of CPU profiles to get to this point where we are using every possible cache layer on the chip, we are using the instruction cache that's on the chip, we're fully utilizing all the integer math operations, we're fully utilizing floating point operations, we're beating the hell out of the memory controller, and so all of this factors in.

The final result that we use is computed with Blake 2B, which is a cryptographic hash algorithm that was designed specifically to run well on CPUs. And then for even larger computations we still use AES, and the reason this works well is because most modern CPUs now have hardware accelerated AES. If it wasn't for that, then that really wouldn't be an option.

I think I'm gonna have to race through the rest of this.

There is a problem where if you try to analyze a program, it would be possible for you to design an implementation where you heavily optimize some of the operations and you avoid some of the other ones, and you can scan a generated program and say, does this have my slow operations in it, if it does I'm gonna ignore this program, if it's got my faster operations in it, I will execute it and get a faster hash result. So we've had to address this problem by chaining multiple programs to force any implementation to either do all or nothing, you either run the entire programs or you skip it.

One of the key constraints was that verification time had to be about the same as for Cryptonight. And because of that, this puts a strong upper bound on the complexity of the programs we can generate.

The amount of memory we use is designed to kind of force you to use off chip memory, so we're using over two gigabytes, in practice it's possible to build chips with two gigs of memory on them today but it's fairly expensive so we anticipate that this is good enough for the next couple years. But again, down the road we'll probably increase these sizes.

We have a light mode that doesn't require the two gigs of RAM, so it only requires 256 megs of RAM, it runs about eight times slower than the full mode and if we were to reduce the memory even further, the next step down would by 128 megs and it would be 3700 times slower so there's a time memory tradeoff and we picked the sweet spot there.

OK, so the current status of the code, it's ready to run in monerod right now for x86. We still need to implement it on ARM. GPU work is progressing, we have support for Nvidia, CUDA, there is kinda of an OpenCL version for AMD GPUs, it's not generic OpenCL, it still uses a lot of AMD specific assembler code. We've got four security audits, one of them was already completed, two of them are in progress right now, and actually the last one begins tomorrow.

Here's some of the hash rates we've observed so far. We're looking to collect more benchmark data from anyone who's happy to run.

And there we are.
